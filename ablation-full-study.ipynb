{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddd8e2d",
   "metadata": {
    "papermill": {
     "duration": 0.011248,
     "end_time": "2022-12-09T17:08:16.210175",
     "exception": false,
     "start_time": "2022-12-09T17:08:16.198927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543c6614",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:16.237312Z",
     "iopub.status.busy": "2022-12-09T17:08:16.235818Z",
     "iopub.status.idle": "2022-12-09T17:08:23.935641Z",
     "shell.execute_reply": "2022-12-09T17:08:23.934617Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 7.718108,
     "end_time": "2022-12-09T17:08:23.939062",
     "exception": false,
     "start_time": "2022-12-09T17:08:16.220954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------IMPORTING PACKAGES---------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import packages\n",
    "print(\"---------IMPORTING PACKAGES---------\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import image\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPool2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Suppress keras warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "random.seed(432)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0dbf6",
   "metadata": {
    "papermill": {
     "duration": 0.006988,
     "end_time": "2022-12-09T17:08:23.953895",
     "exception": false,
     "start_time": "2022-12-09T17:08:23.946907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Define the changeable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f35dc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:23.970102Z",
     "iopub.status.busy": "2022-12-09T17:08:23.969305Z",
     "iopub.status.idle": "2022-12-09T17:08:23.977892Z",
     "shell.execute_reply": "2022-12-09T17:08:23.977040Z"
    },
    "papermill": {
     "duration": 0.019492,
     "end_time": "2022-12-09T17:08:23.980467",
     "exception": false,
     "start_time": "2022-12-09T17:08:23.960975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Static\n",
    "input_folder = \"/kaggle/input/\"\n",
    "output_folder = \"/kaggle/working/\"\n",
    "\n",
    "\n",
    "# Dynamic (to experiment with)\n",
    "n_epochs = 60\n",
    "batch_size = 128 # How big of a set of data to use for every training epoch\n",
    "verbose = 0 # 0-> dont print model training info, 1-> print them\n",
    "act_fun = \"\"\n",
    "loss_fun = \"\"\n",
    "optimizer = \"\"\n",
    "learn_rate = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea747d",
   "metadata": {
    "papermill": {
     "duration": 0.006538,
     "end_time": "2022-12-09T17:08:23.994270",
     "exception": false,
     "start_time": "2022-12-09T17:08:23.987732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create a file to save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2989ec52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:24.010331Z",
     "iopub.status.busy": "2022-12-09T17:08:24.009868Z",
     "iopub.status.idle": "2022-12-09T17:08:24.018747Z",
     "shell.execute_reply": "2022-12-09T17:08:24.017392Z"
    },
    "papermill": {
     "duration": 0.02185,
     "end_time": "2022-12-09T17:08:24.023568",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.001718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the excel file where I'll save all the test results (metrics)\n",
    "\n",
    "# TODO: add AUC, specificity, computational complexity, avg training time per epoch. \n",
    "\n",
    "with open(\"./results.csv\", \"w\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    # Create a row with the column names:\n",
    "    filewriter.writerow([\"model_name\",\n",
    "                         \"removed_layer\", \n",
    "                         \"test_loss\",\n",
    "                         \"test_accuracy\",\n",
    "                         \"wa_precision\", # wa = weighted average\n",
    "                         \"wa_recall\",\n",
    "                         \"wa_f1_score\",\n",
    "                         \"wrong_preds\",\n",
    "                         \"total_preds\",\n",
    "                         \"batch_size\",\n",
    "                         \"num_train\", # number of images in the training set\n",
    "                         \"num_val\", # number of images in the validation set\n",
    "                         \"num_test\", # number of images in the test set\n",
    "                         \"num_epochs_set\", # cap set for num epochs\n",
    "                         \"num_epochs_run\" # epoch where it stopped automatically\n",
    "                        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9a528",
   "metadata": {
    "papermill": {
     "duration": 0.005088,
     "end_time": "2022-12-09T17:08:24.033687",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.028599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa30e93",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:24.045446Z",
     "iopub.status.busy": "2022-12-09T17:08:24.045145Z",
     "iopub.status.idle": "2022-12-09T17:08:24.061817Z",
     "shell.execute_reply": "2022-12-09T17:08:24.060915Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.02477,
     "end_time": "2022-12-09T17:08:24.063738",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.038968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39280813/visualization-of-convolutional-layer-in-keras-model\n",
    "# Merdan Memtimin https://stackoverflow.com/users/6565409/merdan-memtimin\n",
    "def plot_conv_weights(model, layer_name):\n",
    "    W = model.get_layer(name=layer_name).get_weights()[0]\n",
    "    if len(W.shape) == 4:\n",
    "        W = np.squeeze(W)\n",
    "        W = W.reshape((W.shape[0], W.shape[1], W.shape[2]*W.shape[3])) \n",
    "        fig, axs = plt.subplots(5,5, figsize=(8,8))\n",
    "        fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "        axs = axs.ravel()\n",
    "        for i in range(25):\n",
    "            axs[i].imshow(W[:,:,i])\n",
    "            axs[i].set_title(str(i))\n",
    "            \n",
    "            \n",
    "def load_metadata(data_folder, train_file, validation_file, test_file, column_names):\n",
    "    \"\"\"Reads the .csv files containing metadata (e.g. file location, class label) about the data images.\n",
    "    \n",
    "    Parameters:\n",
    "    data_folder (str): file path to the folder containing the csv file.\n",
    "    train_file (str): file name of the file containing metadata for the training set.\n",
    "    validation_file (str): file name of the file containing metadata for the validation set.\n",
    "    test_file (str): file name of the file containing metadata for the test set.\n",
    "    column_names (list): array containing the column names for the csv files\n",
    "    \n",
    "    Returns:\n",
    "    the read trainig, validation, and test files.\n",
    "    \"\"\"\n",
    "    training_df = pd.read_csv(data_folder + train_file, names = column_names, skiprows=1)\n",
    "    validation_df = pd.read_csv(data_folder + validation_file, names = column_names, skiprows=1)\n",
    "    test_df = pd.read_csv(data_folder + test_file, names = column_names, skiprows=1)\n",
    "    \n",
    "    return training_df, validation_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def precision(confusion_matrix):\n",
    "    \"\"\"Given a confusion matrix, calculates and returns precision for all classes\"\"\"\n",
    "    precision_list = []\n",
    "    for i in range(confusion_matrix.shape[1]):\n",
    "        current_col = confusion_matrix[:,[i]]\n",
    "        current_el = confusion_matrix[i,[i]]\n",
    "        current_precision = (float(current_el/sum(current_col)[0]))\n",
    "        current_precision = round(current_precision, 4)\n",
    "        precision_list.append(current_precision)\n",
    "    return precision_list\n",
    "\n",
    "def recall(confusion_matrix):\n",
    "    \"\"\"Given a confusion matrix, calculates and returns recall for all classes\"\"\"\n",
    "    recall_list = []\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        current_row = confusion_matrix[i,:]\n",
    "        current_el = confusion_matrix[i,i]\n",
    "        current_recall = round(current_el/sum(current_row), 4)\n",
    "        recall_list.append(current_recall)\n",
    "    return(recall_list)\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    \"\"\"Given a list of precision and recall values for a certain matrix,\n",
    "    returns f-1 values for each class\"\"\"\n",
    "    f1_scores = []\n",
    "    if len(precision) == len(recall):\n",
    "        for i in range(len(precision)):\n",
    "            current_f1 = 2*(precision[i]*recall[i])/(precision[i]+recall[i])\n",
    "            f1_scores.append(round(current_f1, 4))\n",
    "    else:\n",
    "        print(\"Error. Precision and Recall lists should be of equal lengths.\")\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6aace6",
   "metadata": {
    "papermill": {
     "duration": 0.004347,
     "end_time": "2022-12-09T17:08:24.074454",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.070107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c593c7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:24.084924Z",
     "iopub.status.busy": "2022-12-09T17:08:24.084646Z",
     "iopub.status.idle": "2022-12-09T17:08:24.150918Z",
     "shell.execute_reply": "2022-12-09T17:08:24.149321Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.074632,
     "end_time": "2022-12-09T17:08:24.153824",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.079192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------LOADING DATA---------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------LOADING DATA---------\")\n",
    "dataset_folder = input_folder + \"eurosat-dataset/\" + \"EuroSAT/\"\n",
    "column_names =  [\"id\", \"path\", \"class_id\", \"class_name\"]\n",
    "\n",
    "training_df, validation_df, test_df = load_metadata(dataset_folder,\n",
    "                                                    \"train.csv\",\n",
    "                                                    \"validation.csv\",\n",
    "                                                    \"test.csv\",\n",
    "                                                    column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd5797",
   "metadata": {
    "papermill": {
     "duration": 0.004608,
     "end_time": "2022-12-09T17:08:24.163452",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.158844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edccfa6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:24.181433Z",
     "iopub.status.busy": "2022-12-09T17:08:24.180989Z",
     "iopub.status.idle": "2022-12-09T17:08:24.188190Z",
     "shell.execute_reply": "2022-12-09T17:08:24.187050Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.020196,
     "end_time": "2022-12-09T17:08:24.190780",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.170584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The data is already normalized so that pixels values range from 0,255. We can rescale pixel value between\n",
    "# [0,1] by divinding by 255.\n",
    "\n",
    "rescaling_factor = 1. / 255\n",
    "# Generate batches of tensor image data by setting up the generators.\n",
    "# If you want to perform augmentation, add parameters to the training_generatorF\n",
    "training_generator = ImageDataGenerator(rescale=rescaling_factor)\n",
    "test_generator = ImageDataGenerator(rescale=rescaling_factor)\n",
    "\n",
    "# Size of input data is 64x64 pixels\n",
    "image_width, image_height = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119496b",
   "metadata": {
    "papermill": {
     "duration": 0.004962,
     "end_time": "2022-12-09T17:08:24.200391",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.195429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Build train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e608692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:24.212787Z",
     "iopub.status.busy": "2022-12-09T17:08:24.211138Z",
     "iopub.status.idle": "2022-12-09T17:08:52.422369Z",
     "shell.execute_reply": "2022-12-09T17:08:52.420454Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 28.220396,
     "end_time": "2022-12-09T17:08:52.425511",
     "exception": false,
     "start_time": "2022-12-09T17:08:24.205115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set.\n",
      "Found 18900 validated image filenames belonging to 10 classes.\n",
      "\n",
      "Building validation set.\n",
      "Found 5400 validated image filenames belonging to 10 classes.\n",
      "\n",
      "Building test set\n",
      "Found 2700 validated image filenames belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Building training set.\")\n",
    "training_set = training_generator.flow_from_dataframe(\n",
    "    dataframe=training_df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"class_name\",\n",
    "    directory=dataset_folder,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False, # for replicability. Anyways they are already shuffled.\n",
    ")\n",
    "\n",
    "print(\"\\nBuilding validation set.\")\n",
    "validation_set = test_generator.flow_from_dataframe(\n",
    "    dataframe=validation_df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"class_name\",\n",
    "    directory=dataset_folder,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"\\nBuilding test set\")\n",
    "test_set = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"class_name\",\n",
    "    directory=dataset_folder,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_name_list = np.array(list(training_set.class_indices.keys()))\n",
    "labels, count = np.unique(training_set.classes, return_counts=True)\n",
    "\n",
    "class_count = len(class_name_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54008e",
   "metadata": {
    "papermill": {
     "duration": 0.00469,
     "end_time": "2022-12-09T17:08:52.436065",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.431375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6c5bbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:52.447260Z",
     "iopub.status.busy": "2022-12-09T17:08:52.446398Z",
     "iopub.status.idle": "2022-12-09T17:08:52.452354Z",
     "shell.execute_reply": "2022-12-09T17:08:52.451209Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.014487,
     "end_time": "2022-12-09T17:08:52.455295",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.440808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------START OF ABLATION EXPERIMENTS---------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------START OF ABLATION EXPERIMENTS---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336ef97",
   "metadata": {
    "papermill": {
     "duration": 0.004638,
     "end_time": "2022-12-09T17:08:52.465505",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.460867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Functions\n",
    "### Compile and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca4b18ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:52.477089Z",
     "iopub.status.busy": "2022-12-09T17:08:52.476264Z",
     "iopub.status.idle": "2022-12-09T17:08:52.483805Z",
     "shell.execute_reply": "2022-12-09T17:08:52.483011Z"
    },
    "papermill": {
     "duration": 0.015515,
     "end_time": "2022-12-09T17:08:52.485904",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.470389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(model, model_name):\n",
    "    \"\"\" Given a model architecture, the function compiles and fits the model\n",
    "    \n",
    "    model: model architecture\n",
    "    model_name: \"str\", name used as model description in the output file\n",
    "    \"\"\"\n",
    "    # Path to save the model\n",
    "    model_path = \"/kaggle/working/eurosat_rgb_\" + model_name + \".h5\"\n",
    "    # Save the weigths of the best model\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path, monitor=\"val_loss\", save_best_only=True)\n",
    "    # Reduce learning rate when val_loss stops improving for 5 epochs.\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5)\n",
    "    # Stop training if a val_loss has stopped improving for 5 epochs.\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=verbose)\n",
    "    callback_list = [checkpoint, early_stopping, reduce_lr]\n",
    "    \n",
    "    print(\"---------MODEL COMPILE---------\")\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=Adam(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    print(\"---------MODEL FIT---------\")\n",
    "    history = model.fit(\n",
    "        training_set,\n",
    "        validation_data=validation_set,\n",
    "        callbacks=callback_list,\n",
    "        epochs=n_epochs,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffd8bd",
   "metadata": {
    "papermill": {
     "duration": 0.005118,
     "end_time": "2022-12-09T17:08:52.496600",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.491482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74547689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:52.508235Z",
     "iopub.status.busy": "2022-12-09T17:08:52.507943Z",
     "iopub.status.idle": "2022-12-09T17:08:52.518287Z",
     "shell.execute_reply": "2022-12-09T17:08:52.517424Z"
    },
    "papermill": {
     "duration": 0.018683,
     "end_time": "2022-12-09T17:08:52.520489",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.501806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_eval_metrics(history, model, test_set):\n",
    "    print(\"---------MODEL EVALUATE---------\")\n",
    "    n_ran_epochs = len(history.history['loss'])\n",
    "    loss, accuracy = model.evaluate(test_set, verbose=verbose)\n",
    "    test_set.reset()\n",
    "    predicted_probabilities = model.predict(test_set, steps=test_set.n, verbose=verbose)\n",
    "    predicted_class_ids = np.argmax(predicted_probabilities, axis=1)\n",
    "    report = metrics.classification_report(test_set.labels, predicted_class_ids, output_dict=True)\n",
    "    wa_prec = report[\"weighted avg\"][\"precision\"]\n",
    "    wa_rec = report[\"weighted avg\"][\"recall\"]\n",
    "    wa_f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "    cf_matrix = confusion_matrix(test_set.labels, predicted_class_ids)\n",
    "    confusion_matrix_df = pd.DataFrame(\n",
    "        data=cf_matrix, \n",
    "        index=class_name_list, \n",
    "        columns=class_name_list\n",
    "    )\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"label\": test_set.labels, \n",
    "            \"prediction\": predicted_class_ids, \n",
    "            \"path\": test_set.filepaths\n",
    "        }\n",
    "        )\n",
    "\n",
    "    mistakes = results_df[\"label\"] != results_df[\"prediction\"]\n",
    "    mistakes_df = results_df[mistakes]\n",
    "    wrong_preds = len(mistakes_df)\n",
    "    tot_preds = len(results_df)\n",
    "\n",
    "    print(metrics.classification_report(test_set.labels, predicted_class_ids, digits=4))\n",
    "    \n",
    "    return n_ran_epochs, loss, accuracy, wa_prec, wa_rec, wa_f1, cf_matrix, confusion_matrix_df, wrong_preds, tot_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39834497",
   "metadata": {
    "papermill": {
     "duration": 0.005136,
     "end_time": "2022-12-09T17:08:52.530670",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.525534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ca8094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:52.542180Z",
     "iopub.status.busy": "2022-12-09T17:08:52.541902Z",
     "iopub.status.idle": "2022-12-09T17:08:52.550415Z",
     "shell.execute_reply": "2022-12-09T17:08:52.549299Z"
    },
    "papermill": {
     "duration": 0.016755,
     "end_time": "2022-12-09T17:08:52.552571",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.535816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_plots(history, output_folder, model_name, confusion_matrix_df):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # Loss\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Loss - \" + model_name)\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"accuracy\"])\n",
    "    plt.plot(history.history[\"val_accuracy\"])\n",
    "    plt.title(\"Accuracy - \" + model_name)\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "    #plt.show()\n",
    "    plt.savefig(output_folder + model_name + \"_training_curves.png\")\n",
    "    \n",
    "    # CONFUSION MATRIX IMAGE\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(confusion_matrix_df, annot=True, cmap=\"Blues\", annot_kws={\"size\": 9}, fmt=\"g\")\n",
    "    plt.ylabel(\"Label\")\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.title(\"Confusion matrix - \" + model_name)\n",
    "    #plt.show()\n",
    "    plt.savefig(output_folder + model_name + \"_confusion.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bc273",
   "metadata": {
    "papermill": {
     "duration": 0.004937,
     "end_time": "2022-12-09T17:08:52.562656",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.557719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save numerical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9521bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T17:08:52.575048Z",
     "iopub.status.busy": "2022-12-09T17:08:52.574298Z",
     "iopub.status.idle": "2022-12-09T17:08:52.580175Z",
     "shell.execute_reply": "2022-12-09T17:08:52.579248Z"
    },
    "papermill": {
     "duration": 0.014494,
     "end_time": "2022-12-09T17:08:52.582213",
     "exception": false,
     "start_time": "2022-12-09T17:08:52.567719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_results_to_csv(csv_row):\n",
    "    print(\"---------SAVE RESULTS---------\")\n",
    "    \"\"\"Append the results to the csv file\n",
    "    csv_row: list, contains the information to append to the csv\"\"\"\n",
    "    \n",
    "    with open(\"./results.csv\", 'a+', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        # Create a row with the column names:\n",
    "        filewriter.writerow(csv_row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.493646,
   "end_time": "2022-12-09T17:08:56.148737",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-09T17:08:07.655091",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
