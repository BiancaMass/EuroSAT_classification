{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c004bbe4",
   "metadata": {
    "papermill": {
     "duration": 0.008794,
     "end_time": "2022-12-11T12:15:45.859660",
     "exception": false,
     "start_time": "2022-12-11T12:15:45.850866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8aafa22",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-12-11T12:15:45.877826Z",
     "iopub.status.busy": "2022-12-11T12:15:45.876668Z",
     "iopub.status.idle": "2022-12-11T12:15:55.391546Z",
     "shell.execute_reply": "2022-12-11T12:15:55.390211Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 9.527311,
     "end_time": "2022-12-11T12:15:55.394770",
     "exception": false,
     "start_time": "2022-12-11T12:15:45.867459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------IMPORTING PACKAGES---------\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "print(\"---------IMPORTING PACKAGES---------\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "import keras\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import image\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPool2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, Adamax, SGD\n",
    "\n",
    "# Suppress keras warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "random.seed(432)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50992d9d",
   "metadata": {
    "papermill": {
     "duration": 0.007321,
     "end_time": "2022-12-11T12:15:55.410077",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.402756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Define the changeable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e319d9ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:15:55.427875Z",
     "iopub.status.busy": "2022-12-11T12:15:55.427109Z",
     "iopub.status.idle": "2022-12-11T12:15:55.450881Z",
     "shell.execute_reply": "2022-12-11T12:15:55.449407Z"
    },
    "papermill": {
     "duration": 0.036182,
     "end_time": "2022-12-11T12:15:55.453928",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.417746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "################### FIXED VARIABLES #####################\n",
    "#########################################################\n",
    "input_folder = \"/kaggle/input/\"\n",
    "#input_folder_undersampled = input_folder+\"eurosat-undersampled\"\n",
    "output_folder = \"/kaggle/working/\"\n",
    "verbose = 0 # 0-> dont print training info, 1-> print them\n",
    "n_epochs = 2\n",
    "\n",
    "# Model parameters\n",
    "pooling_kernel_size = (2,2)\n",
    "stride = (1,1)\n",
    "#########################################################\n",
    "\n",
    "\n",
    "\n",
    "##############################################################\n",
    "################### HYPERPARAMETER OPTIONS ###################\n",
    "##############################################################\n",
    "# Element [0] is default\n",
    "batch_size_opts = [128, 256, 64] # How big of a set of data to use for every training epoch\n",
    "loss_fun_opts = [\"categorical_crossentropy\",\n",
    "                 \"kl_divergence\",\n",
    "                 \"mean_absolute_error\",\n",
    "                 \"mean_squared_logarithmic_error\"]\n",
    "act_fun_opts = [\"relu\", \"elu\"]\n",
    "learn_rate_opts = [0.0001, 0.0007, 0.001]\n",
    "\n",
    "learn_rate = learn_rate_opts[0] # 2g # FIXED FOR NOW\n",
    "\n",
    "optmzer_opts = [Adam(learning_rate=learn_rate),\n",
    "                Nadam(learning_rate=learn_rate),\n",
    "                Adamax(learning_rate=learn_rate),\n",
    "                SGD(learning_rate=learn_rate)]\n",
    "\n",
    "# Model Architecture:\n",
    "n_convolutional_layers_opts = [2, 4, 5, 8]\n",
    "n_pooling_layers_opts = n_convolutional_layers_opts\n",
    "filter_size_opts = [(2,2), (3,3), (5,5)] # kernel size of the convolutional layer\n",
    "\n",
    "filter_number_opts = [[16, 32],\n",
    "                      [16, 16, 32, 32],\n",
    "                      [16, 32, 32, 32, 64],\n",
    "                      [16, 16, 32, 32, 64, 64, 128, 128]] # DEPENDS ON NUMBER OF CONVOLUTIONAL LAYERS \n",
    "##############################################################\n",
    "\n",
    "\n",
    "\n",
    "##############################################################\n",
    "################### HYPERPARAMETER VALUES ####################\n",
    "##############################################################\n",
    "batch_size = batch_size_opts[0] # 2e\n",
    "loss_fun = loss_fun_opts[0] # FIXED FOR NOW\n",
    "act_fun = act_fun_opts[0] # 2d\n",
    "optmzer = optmzer_opts[0] # 2f\n",
    "n_convolutional_layers = n_convolutional_layers_opts[0] # 2a\n",
    "n_pooling_layers = n_pooling_layers_opts[0] # 2a\n",
    "filter_size = filter_size_opts[0] # 2b\n",
    "filters_number = [] # 2c # TODO DEPENDS ON NUMBER OF CONV LAYER. GENERAL RULE: INCREASING.\n",
    "\n",
    "\n",
    "params = [n_epochs,\n",
    "          batch_size,\n",
    "          loss_fun, \n",
    "          act_fun,\n",
    "          learn_rate, \n",
    "          optmzer,\n",
    "          n_convolutional_layers,\n",
    "          n_pooling_layers,\n",
    "          filter_size,\n",
    "          filters_number\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4929a",
   "metadata": {
    "papermill": {
     "duration": 0.007242,
     "end_time": "2022-12-11T12:15:55.469059",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.461817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For the number of filters, it makes sense to increase gradually. So that you could leave as a more fixed variable. \n",
    "start with few to get the rough features (line direction) and increase afterwards to pick up on the more abstract details. But don't go too big, else comp complexity spikes up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545f14c",
   "metadata": {
    "papermill": {
     "duration": 0.007307,
     "end_time": "2022-12-11T12:15:55.485714",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.478407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create a fine to save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3232e6b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:15:55.503575Z",
     "iopub.status.busy": "2022-12-11T12:15:55.502869Z",
     "iopub.status.idle": "2022-12-11T12:15:55.511243Z",
     "shell.execute_reply": "2022-12-11T12:15:55.510284Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.020485,
     "end_time": "2022-12-11T12:15:55.513932",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.493447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the excel file where I'll save all the test results (metrics)\n",
    "\n",
    "# TODO: add computational complexity\n",
    "\n",
    "with open(\"./results.csv\", \"w\") as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    # Create a row with the column names:\n",
    "    filewriter.writerow([\"model_ID\",\n",
    "                         \"model_name\", \n",
    "                         \"n_epochs\", # params[0]\n",
    "                         \"batch_size\", # params[1]\n",
    "                         \"loss_function\", # params[2]\n",
    "                         \"activation_fun\", # params[3]\n",
    "                         \"learning_rate\", # params[4]\n",
    "                         \"optimizer\", # params[5]\n",
    "                         \"num_conv_layers\", # params[6]\n",
    "                         \"num_pooling_layers\", # params[7]\n",
    "                         \"kernel_size\", # params[8]\n",
    "                         \"filter_number\", # params[9]\n",
    "                         \"train_time_per_epoch\",\n",
    "                         \"num_epochs_run\", # epoch where it stopped automatically\n",
    "                         \"test_loss\",\n",
    "                         \"test_accuracy\",\n",
    "                         \"test_auc\",\n",
    "                         \"wa_precision\", # wa = weighted average\n",
    "                         \"wa_recall\",\n",
    "                         \"wa_f1_score\",\n",
    "                         \"wrong_preds\",\n",
    "                         \"total_preds\",\n",
    "                         \"num_train\", # number of images in the training set\n",
    "                         \"num_val\", # number of images in the validation set\n",
    "                         \"num_test\" # number of images in the test set  \n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f58d2",
   "metadata": {
    "papermill": {
     "duration": 0.007368,
     "end_time": "2022-12-11T12:15:55.529478",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.522110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badb8e1b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-11T12:15:55.547227Z",
     "iopub.status.busy": "2022-12-11T12:15:55.546522Z",
     "iopub.status.idle": "2022-12-11T12:15:55.565115Z",
     "shell.execute_reply": "2022-12-11T12:15:55.563880Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.030857,
     "end_time": "2022-12-11T12:15:55.568006",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.537149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39280813/visualization-of-convolutional-layer-in-keras-model\n",
    "# Merdan Memtimin https://stackoverflow.com/users/6565409/merdan-memtimin\n",
    "def plot_conv_weights(model, layer_name):\n",
    "    W = model.get_layer(name=layer_name).get_weights()[0]\n",
    "    if len(W.shape) == 4:\n",
    "        W = np.squeeze(W)\n",
    "        W = W.reshape((W.shape[0], W.shape[1], W.shape[2]*W.shape[3])) \n",
    "        fig, axs = plt.subplots(5,5, figsize=(8,8))\n",
    "        fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "        axs = axs.ravel()\n",
    "        for i in range(25):\n",
    "            axs[i].imshow(W[:,:,i])\n",
    "            axs[i].set_title(str(i))\n",
    "            \n",
    "            \n",
    "def load_metadata(data_folder, train_file, validation_file, test_file, column_names):\n",
    "    \"\"\"Reads the .csv files containing metadata (e.g. file location, class label) about the data images.\n",
    "    \n",
    "    Parameters:\n",
    "    data_folder (str): file path to the folder containing the csv file.\n",
    "    train_file (str): file name of the file containing metadata for the training set.\n",
    "    validation_file (str): file name of the file containing metadata for the validation set.\n",
    "    test_file (str): file name of the file containing metadata for the test set.\n",
    "    column_names (list): array containing the column names for the csv files\n",
    "    \n",
    "    Returns:\n",
    "    the read trainig, validation, and test files.\n",
    "    \"\"\"\n",
    "    training_df = pd.read_csv(data_folder + train_file, names = column_names, skiprows=1)\n",
    "    validation_df = pd.read_csv(data_folder + validation_file, names = column_names, skiprows=1)\n",
    "    test_df = pd.read_csv(data_folder + test_file, names = column_names, skiprows=1)\n",
    "    \n",
    "    return training_df, validation_df, test_df\n",
    "\n",
    "\n",
    "def flatten(d):\n",
    "    v = [[i] if not isinstance(i, list) else flatten(i) for i in d]\n",
    "    return [i for b in v for i in b]\n",
    "\n",
    "\n",
    "def precision(confusion_matrix):\n",
    "    \"\"\"Given a confusion matrix, calculates and returns precision for all classes\"\"\"\n",
    "    precision_list = []\n",
    "    for i in range(confusion_matrix.shape[1]):\n",
    "        current_col = confusion_matrix[:,[i]]\n",
    "        current_el = confusion_matrix[i,[i]]\n",
    "        current_precision = (float(current_el/sum(current_col)[0]))\n",
    "        current_precision = round(current_precision, 4)\n",
    "        precision_list.append(current_precision)\n",
    "    return precision_list\n",
    "\n",
    "def recall(confusion_matrix):\n",
    "    \"\"\"Given a confusion matrix, calculates and returns recall for all classes\"\"\"\n",
    "    recall_list = []\n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        current_row = confusion_matrix[i,:]\n",
    "        current_el = confusion_matrix[i,i]\n",
    "        current_recall = round(current_el/sum(current_row), 4)\n",
    "        recall_list.append(current_recall)\n",
    "    return(recall_list)\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    \"\"\"Given a list of precision and recall values for a certain matrix,\n",
    "    returns f-1 values for each class\"\"\"\n",
    "    f1_scores = []\n",
    "    if len(precision) == len(recall):\n",
    "        for i in range(len(precision)):\n",
    "            current_f1 = 2*(precision[i]*recall[i])/(precision[i]+recall[i])\n",
    "            f1_scores.append(round(current_f1, 4))\n",
    "    else:\n",
    "        print(\"Error. Precision and Recall lists should be of equal lengths.\")\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb62ed2",
   "metadata": {
    "papermill": {
     "duration": 0.007689,
     "end_time": "2022-12-11T12:15:55.584199",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.576510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277d906f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:15:55.602452Z",
     "iopub.status.busy": "2022-12-11T12:15:55.601934Z",
     "iopub.status.idle": "2022-12-11T12:15:55.702159Z",
     "shell.execute_reply": "2022-12-11T12:15:55.700752Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.113036,
     "end_time": "2022-12-11T12:15:55.705325",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.592289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------LOADING DATA---------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------LOADING DATA---------\")\n",
    "dataset_folder = input_folder + \"eurosat-dataset/\" + \"EuroSAT/\"\n",
    "column_names =  [\"id\", \"path\", \"class_id\", \"class_name\"]\n",
    "\n",
    "training_df, validation_df, test_df = load_metadata(dataset_folder,\n",
    "                                                    \"train.csv\",\n",
    "                                                    \"validation.csv\",\n",
    "                                                    \"test.csv\",\n",
    "                                                    column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c004562",
   "metadata": {
    "papermill": {
     "duration": 0.007714,
     "end_time": "2022-12-11T12:15:55.720909",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.713195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c01ce07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:15:55.739292Z",
     "iopub.status.busy": "2022-12-11T12:15:55.738331Z",
     "iopub.status.idle": "2022-12-11T12:15:55.745922Z",
     "shell.execute_reply": "2022-12-11T12:15:55.744715Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.019478,
     "end_time": "2022-12-11T12:15:55.748431",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.728953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The data is already normalized so that pixels values range from 0,255. We can rescale pixel value between\n",
    "# [0,1] by divinding by 255.\n",
    "\n",
    "rescaling_factor = 1. / 255\n",
    "# Generate batches of tensor image data by setting up the generators.\n",
    "# If you want to perform augmentation, add parameters to the training_generatorF\n",
    "training_generator = ImageDataGenerator(rescale=rescaling_factor)\n",
    "test_generator = ImageDataGenerator(rescale=rescaling_factor)\n",
    "\n",
    "# Size of input data is 64x64 pixels\n",
    "image_width, image_height = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bccfa78",
   "metadata": {
    "papermill": {
     "duration": 0.007557,
     "end_time": "2022-12-11T12:15:55.763808",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.756251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Build train, val, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de6fceee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:15:55.781684Z",
     "iopub.status.busy": "2022-12-11T12:15:55.781244Z",
     "iopub.status.idle": "2022-12-11T12:16:52.026784Z",
     "shell.execute_reply": "2022-12-11T12:16:52.025416Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 56.257906,
     "end_time": "2022-12-11T12:16:52.029666",
     "exception": false,
     "start_time": "2022-12-11T12:15:55.771760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set.\n",
      "Found 18900 validated image filenames belonging to 10 classes.\n",
      "\n",
      "Building validation set.\n",
      "Found 5400 validated image filenames belonging to 10 classes.\n",
      "\n",
      "Building test set\n",
      "Found 2700 validated image filenames belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Building training set.\")\n",
    "training_set = training_generator.flow_from_dataframe(\n",
    "    dataframe=training_df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"class_name\",\n",
    "    directory=dataset_folder,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False, # for replicability. Anyways they are already shuffled.\n",
    ")\n",
    "\n",
    "print(\"\\nBuilding validation set.\")\n",
    "validation_set = test_generator.flow_from_dataframe(\n",
    "    dataframe=validation_df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"class_name\",\n",
    "    directory=dataset_folder,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"\\nBuilding test set\")\n",
    "test_set = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"class_name\",\n",
    "    directory=dataset_folder,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class_name_list = np.array(list(training_set.class_indices.keys()))\n",
    "labels, count = np.unique(training_set.classes, return_counts=True)\n",
    "\n",
    "class_count = len(class_name_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2373e8",
   "metadata": {
    "papermill": {
     "duration": 0.00789,
     "end_time": "2022-12-11T12:16:52.046156",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.038266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc5612b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.064804Z",
     "iopub.status.busy": "2022-12-11T12:16:52.064373Z",
     "iopub.status.idle": "2022-12-11T12:16:52.071140Z",
     "shell.execute_reply": "2022-12-11T12:16:52.069409Z"
    },
    "papermill": {
     "duration": 0.019349,
     "end_time": "2022-12-11T12:16:52.074026",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.054677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------START OF ABLATION EXPERIMENTS---------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------START OF ABLATION EXPERIMENTS---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a2aff",
   "metadata": {
    "papermill": {
     "duration": 0.007876,
     "end_time": "2022-12-11T12:16:52.089969",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.082093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Functions\n",
    "### Compile and Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37a745",
   "metadata": {
    "papermill": {
     "duration": 0.00777,
     "end_time": "2022-12-11T12:16:52.105778",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.098008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define Time History class to record time taken per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ea46e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.124531Z",
     "iopub.status.busy": "2022-12-11T12:16:52.123725Z",
     "iopub.status.idle": "2022-12-11T12:16:52.130489Z",
     "shell.execute_reply": "2022-12-11T12:16:52.129369Z"
    },
    "papermill": {
     "duration": 0.01943,
     "end_time": "2022-12-11T12:16:52.133354",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.113924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad73643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.152211Z",
     "iopub.status.busy": "2022-12-11T12:16:52.151727Z",
     "iopub.status.idle": "2022-12-11T12:16:52.162791Z",
     "shell.execute_reply": "2022-12-11T12:16:52.161821Z"
    },
    "papermill": {
     "duration": 0.023147,
     "end_time": "2022-12-11T12:16:52.165154",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.142007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(model, model_name, params):\n",
    "    \"\"\"\n",
    "    Given a model architecture, the function compiles and fits the model.\n",
    "    \n",
    "    model: model architecture\n",
    "    model_name: \"str\", name used as model description in the output file\n",
    "    params: an array containing the values for the training parameters, namely:\n",
    "    [n_epochs, batch_size, loss_fun, act_fun, learn_rate, optmzer]\n",
    "    \"\"\"\n",
    "    # Path to save the model\n",
    "    model_path = \"/kaggle/working/eurosat_rgb_\" + model_name + \".h5\"\n",
    "    # Save the weigths of the best model\n",
    "    checkpoint = ModelCheckpoint(filepath=model_path, monitor=\"val_loss\", save_best_only=True)\n",
    "    # Reduce learning rate when val_loss stops improving for 5 epochs.\n",
    "    # reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5)\n",
    "    # Stop training if a val_loss has stopped improving for 5 epochs.\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=verbose)\n",
    "    \n",
    "    time_callback = TimeHistory()\n",
    "    callback_list = [checkpoint, early_stopping, time_callback] #, reduce_lr]\n",
    "    \n",
    "    metrics=['accuracy',\n",
    "             tf.keras.metrics.AUC(from_logits=True)]\n",
    "    \n",
    "    print(\"---------MODEL COMPILE---------\")\n",
    "    model.compile(loss=params[2],\n",
    "                  optimizer=params[5],\n",
    "                  metrics=metrics)\n",
    "\n",
    "    print(\"---------MODEL FIT---------\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        training_set,\n",
    "        validation_data=validation_set,\n",
    "        callbacks=callback_list,\n",
    "        epochs=params[0],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    \n",
    "    times = time_callback.times\n",
    "    avg_time_epoch = sum(times)/len(times)\n",
    "    new_params = params\n",
    "    new_params.append(avg_time_epoch)\n",
    "\n",
    "    return history, new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf07d1",
   "metadata": {
    "papermill": {
     "duration": 0.008302,
     "end_time": "2022-12-11T12:16:52.182219",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.173917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ba3e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.202523Z",
     "iopub.status.busy": "2022-12-11T12:16:52.201639Z",
     "iopub.status.idle": "2022-12-11T12:16:52.216695Z",
     "shell.execute_reply": "2022-12-11T12:16:52.215518Z"
    },
    "papermill": {
     "duration": 0.028157,
     "end_time": "2022-12-11T12:16:52.219668",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.191511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_eval_metrics(history, model, test_set, params):\n",
    "    \"\"\"\n",
    "    Given a trained model and a test set, the function returns model eval metrics.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"---------MODEL EVALUATE---------\")\n",
    "    \n",
    "    n_ran_epochs = len(history.history['loss'])\n",
    "    \n",
    "    loss, accuracy, auc = model.evaluate(test_set, verbose=verbose)\n",
    "    \n",
    "    test_set.reset()\n",
    "    predicted_probabilities = model.predict(test_set, steps=test_set.n, verbose=verbose)\n",
    "    predicted_class_ids = np.argmax(predicted_probabilities, axis=1)\n",
    "    report = metrics.classification_report(test_set.labels, predicted_class_ids, output_dict=True)\n",
    "    wa_prec = report[\"weighted avg\"][\"precision\"]\n",
    "    wa_rec = report[\"weighted avg\"][\"recall\"]\n",
    "    wa_f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "    cf_matrix = confusion_matrix(test_set.labels, predicted_class_ids)\n",
    "    confusion_matrix_df = pd.DataFrame(\n",
    "        data=cf_matrix, \n",
    "        index=class_name_list, \n",
    "        columns=class_name_list\n",
    "    )\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"label\": test_set.labels, \n",
    "            \"prediction\": predicted_class_ids, \n",
    "            \"path\": test_set.filepaths\n",
    "        }\n",
    "        )\n",
    "\n",
    "    mistakes = results_df[\"label\"] != results_df[\"prediction\"]\n",
    "    mistakes_df = results_df[mistakes]\n",
    "    wrong_preds = len(mistakes_df)\n",
    "    tot_preds = len(results_df)\n",
    "\n",
    "    print(metrics.classification_report(test_set.labels, predicted_class_ids, digits=4))\n",
    "    \n",
    "    \n",
    "    eval_metrics = [n_ran_epochs,\n",
    "                   loss,\n",
    "                   accuracy,\n",
    "                   auc,\n",
    "                   wa_prec,\n",
    "                   wa_rec,\n",
    "                   wa_f1,\n",
    "                   wrong_preds,\n",
    "                   tot_preds\n",
    "        ]\n",
    "    \n",
    "    return eval_metrics, cf_matrix, confusion_matrix_df, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09902d73",
   "metadata": {
    "papermill": {
     "duration": 0.008257,
     "end_time": "2022-12-11T12:16:52.236579",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.228322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ce2bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.255670Z",
     "iopub.status.busy": "2022-12-11T12:16:52.255217Z",
     "iopub.status.idle": "2022-12-11T12:16:52.266825Z",
     "shell.execute_reply": "2022-12-11T12:16:52.265608Z"
    },
    "papermill": {
     "duration": 0.024462,
     "end_time": "2022-12-11T12:16:52.269573",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.245111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_plots(history, output_folder, model_name, confusion_matrix_df):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # Loss\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Loss - \" + model_name)\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"accuracy\"])\n",
    "    plt.plot(history.history[\"val_accuracy\"])\n",
    "    plt.title(\"Accuracy - \" + model_name)\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "    #plt.show()\n",
    "    plt.savefig(output_folder + model_name + \"_training_curves.png\")\n",
    "    \n",
    "    # CONFUSION MATRIX IMAGE\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.heatmap(confusion_matrix_df, annot=True, cmap=\"Blues\", annot_kws={\"size\": 9}, fmt=\"g\")\n",
    "    plt.ylabel(\"Label\")\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.title(\"Confusion matrix - \" + model_name)\n",
    "    #plt.show()\n",
    "    plt.savefig(output_folder + model_name + \"_confusion.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d07bdd",
   "metadata": {
    "papermill": {
     "duration": 0.007759,
     "end_time": "2022-12-11T12:16:52.285559",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.277800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save numerical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a5139d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.303774Z",
     "iopub.status.busy": "2022-12-11T12:16:52.303363Z",
     "iopub.status.idle": "2022-12-11T12:16:52.309681Z",
     "shell.execute_reply": "2022-12-11T12:16:52.308312Z"
    },
    "papermill": {
     "duration": 0.01867,
     "end_time": "2022-12-11T12:16:52.312218",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.293548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_results_to_csv(csv_row):\n",
    "    print(\"---------SAVE RESULTS---------\")\n",
    "    \"\"\"Append the results to the csv file\n",
    "    csv_row: list, contains the information to append to the csv\"\"\"\n",
    "    \n",
    "    with open(\"./results.csv\", 'a+', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        # Create a row with the column names:\n",
    "        filewriter.writerow(csv_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63c241",
   "metadata": {
    "papermill": {
     "duration": 0.008127,
     "end_time": "2022-12-11T12:16:52.329357",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.321230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EXPERIMENT 2A.i.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27f9e4a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.348179Z",
     "iopub.status.busy": "2022-12-11T12:16:52.347292Z",
     "iopub.status.idle": "2022-12-11T12:16:52.780169Z",
     "shell.execute_reply": "2022-12-11T12:16:52.779085Z"
    },
    "papermill": {
     "duration": 0.445268,
     "end_time": "2022-12-11T12:16:52.782900",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.337632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- EXPERIMENT 2A.i.1 ----------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              16385000  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 16,414,402\n",
      "Trainable params: 16,414,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Model COMPILE and FIT\\nhistory, new_params = run_model(model, model_name, params)\\nparams = new_params\\n# Model EVALUATE\\neval_metrics, cf_matrix, confusion_matrix_df, results_df = get_eval_metrics(history, model, test_set, params)\\n\\nsave_plots(history, output_folder, model_name, confusion_matrix_df)\\n\\nnew_row = [model_id,\\n           model_name,\\n           params,\\n           eval_metrics,\\n           len(training_df), # number of images in the training set\\n           len(validation_df), # number of images in the validation set\\n           len(test_df), # number of images in the test set\\n          ]\\n\\nnew_row = flatten(new_row)\\n\\nappend_results_to_csv(new_row)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = 2\n",
    "model_id = \"2A.i.1\"\n",
    "model_name = f\"BNET-{model_id}\"\n",
    "\n",
    "print(f\"-------- EXPERIMENT {model_id} ----------\")\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(64,64,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1000, activation=act_fun, kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(class_count, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "# Model COMPILE and FIT\n",
    "history, new_params = run_model(model, model_name, params)\n",
    "params = new_params\n",
    "# Model EVALUATE\n",
    "eval_metrics, cf_matrix, confusion_matrix_df, results_df = get_eval_metrics(history, model, test_set, params)\n",
    "\n",
    "save_plots(history, output_folder, model_name, confusion_matrix_df)\n",
    "\n",
    "new_row = [model_id,\n",
    "           model_name,\n",
    "           params,\n",
    "           eval_metrics,\n",
    "           len(training_df), # number of images in the training set\n",
    "           len(validation_df), # number of images in the validation set\n",
    "           len(test_df), # number of images in the test set\n",
    "          ]\n",
    "\n",
    "new_row = flatten(new_row)\n",
    "\n",
    "append_results_to_csv(new_row)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5502b385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.802727Z",
     "iopub.status.busy": "2022-12-11T12:16:52.802256Z",
     "iopub.status.idle": "2022-12-11T12:16:52.915065Z",
     "shell.execute_reply": "2022-12-11T12:16:52.913090Z"
    },
    "papermill": {
     "duration": 0.126144,
     "end_time": "2022-12-11T12:16:52.918005",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.791861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- EXPERIMENT 2A.i.2 ----------\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               256250    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2510      \n",
      "=================================================================\n",
      "Total params: 324,328\n",
      "Trainable params: 324,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_layers = 4\n",
    "model_id = \"2A.i.2\"\n",
    "model_name = f\"BNET-{model_id}\"\n",
    "\n",
    "print(f\"-------- EXPERIMENT {model_id} ----------\")\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(64,64,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=250, activation=act_fun, kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(class_count, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6154047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:16:52.937926Z",
     "iopub.status.busy": "2022-12-11T12:16:52.936442Z",
     "iopub.status.idle": "2022-12-11T12:16:53.069593Z",
     "shell.execute_reply": "2022-12-11T12:16:53.068264Z"
    },
    "papermill": {
     "duration": 0.146152,
     "end_time": "2022-12-11T12:16:53.072689",
     "exception": false,
     "start_time": "2022-12-11T12:16:52.926537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- EXPERIMENT 2A.i.3 ----------\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               409800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 551,234\n",
      "Trainable params: 551,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_layers = 5\n",
    "model_id = \"2A.i.3\"\n",
    "model_name = f\"BNET-{model_id}\"\n",
    "\n",
    "print(f\"-------- EXPERIMENT {model_id} ----------\")\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(64,64,3),filters=32,kernel_size=(3,3),padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=act_fun))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=200, activation=act_fun, kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(class_count, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a23c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T12:05:08.744342Z",
     "iopub.status.busy": "2022-12-11T12:05:08.743951Z",
     "iopub.status.idle": "2022-12-11T12:05:08.874807Z",
     "shell.execute_reply": "2022-12-11T12:05:08.873539Z",
     "shell.execute_reply.started": "2022-12-11T12:05:08.744310Z"
    },
    "papermill": {
     "duration": 0.009251,
     "end_time": "2022-12-11T12:16:53.091287",
     "exception": false,
     "start_time": "2022-12-11T12:16:53.082036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 82.516292,
   "end_time": "2022-12-11T12:16:56.057237",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-11T12:15:33.540945",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
